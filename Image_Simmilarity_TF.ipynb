{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1QvDNKNMeYVr46COBosk-f0rH0BotJUMq",
      "authorship_tag": "ABX9TyO4x/SA0ZNItuWIMGU9U4R4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MHosseinHashemi/Image_Sim/blob/main/Image_Simmilarity_TF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLGuBu_H_71L"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import functools"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(train_ds, valid_ds), info = tfds.load(\"oxford_flowers102\", split=[\"train\", \"validation\"], as_supervised=True, with_info=True)"
      ],
      "metadata": {
        "id": "chsOvx8HF3um"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_WIDTH = IMG_HEIGHT = 128"
      ],
      "metadata": {
        "id": "J2bOnOddGGYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CLASSES_TO_CONSIDER = list(range(102)) # Only the first 10 classes"
      ],
      "metadata": {
        "id": "L0RApuXOGGar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A function to Prep the images\n",
        "def preprocess_image(image, label, height, width):\n",
        "    image = tf.image.resize_with_crop_or_pad(image, target_height=height, target_width=width)\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    return image, label"
      ],
      "metadata": {
        "id": "5PfvKqYjGGdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A function to filter out unwanted labels ! You should remove it after test training ....\n",
        "def filter_by_classes(img, label):\n",
        "  # Only picks the selected labels\n",
        "  bools = tf.equal(label, CLASSES_TO_CONSIDER)\n",
        "  return tf.reduce_any(bools)"
      ],
      "metadata": {
        "id": "EVivHPZMGGfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new function to prep the\n",
        "partial_preprocess_image = functools.partial(preprocess_image, height=IMG_HEIGHT, width=IMG_WIDTH)\n",
        "train_ds = train_ds.filter(filter_by_classes).map(partial_preprocess_image)\n",
        "valid_ds = valid_ds.filter(filter_by_classes).map(partial_preprocess_image)"
      ],
      "metadata": {
        "id": "jZbWUIWLKMRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_x_y_from_ds(ds):\n",
        "    x, y = [], []\n",
        "    for img, label in ds.cache().as_numpy_iterator():\n",
        "        x.append(img)\n",
        "        y.append(label)\n",
        "\n",
        "    return np.array(x), np.array(y)"
      ],
      "metadata": {
        "id": "anwwa80rNFZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, y_train = get_x_y_from_ds(train_ds)"
      ],
      "metadata": {
        "id": "wkr2EVrAOENB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test, y_test = get_x_y_from_ds(valid_ds)"
      ],
      "metadata": {
        "id": "SnZPkLLGOKAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"number of {len(x_train)} samples\\nnumber of {len(y_train)} samples\\nnumber of {len(x_test)} samples\\nnumber of {len(y_test)} samples\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tiJw2aNmOT-Q",
        "outputId": "5c840baf-a7d3-4bc7-f334-2702f0744cbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of 1020 samples\n",
            "number of 1020 samples\n",
            "number of 1020 samples\n",
            "number of 1020 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model\n",
        "MODEL_URL = \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_s/feature_vector/2\"\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "    tf.keras.layers.RandomRotation(0.3),\n",
        "    hub.KerasLayer(MODEL_URL, trainable=True),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(128, activation=None), # No activation on final dense layer\n",
        "    tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1)) # L2 normalize embeddings\n",
        "])\n",
        "\n",
        "model.build([None, IMG_HEIGHT, IMG_WIDTH, 3])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "USAxUjmLOwmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import random\n",
        "\n",
        "def get_training_batch(images, labels, batch_size=128, n_examples_per_class=4):\n",
        "    examples_per_class = defaultdict(list)\n",
        "    for x, y in zip(images, labels):\n",
        "        examples_per_class[y].append(x)\n",
        "\n",
        "    while True:\n",
        "        batch_X, batch_y = [], []\n",
        "        while len(batch_X) < batch_size:\n",
        "            for cls, examples in examples_per_class.items():\n",
        "                n_sample = min(n_examples_per_class, (batch_size - len(batch_X)))\n",
        "                if n_sample == 0:\n",
        "                    break\n",
        "                samples = random.sample(examples, k=n_sample)\n",
        "                batch_X.extend(samples)\n",
        "                batch_y.extend([cls] * len(samples))\n",
        "\n",
        "        yield np.array(batch_X), np.array(batch_y)"
      ],
      "metadata": {
        "id": "WM07pLS9UqHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install tensorflow_addons"
      ],
      "metadata": {
        "id": "n3sRCSeXUqJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bs = 16\n",
        "n_examples_per_class = 4\n",
        "initial_lr = 0.0001\n",
        "epochs = 80\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=initial_lr),\n",
        "                         loss=tfa.losses.TripletSemiHardLoss())\n",
        "\n",
        "history = model.fit(get_training_batch(images=x_train, labels=y_train, batch_size=bs, n_examples_per_class=n_examples_per_class),\n",
        "                               epochs=epochs,\n",
        "                               steps_per_epoch=len(x_train)//bs,\n",
        "                               validation_data=(x_test, y_test),\n",
        "                               validation_batch_size=bs,\n",
        "                              )"
      ],
      "metadata": {
        "id": "0LCpHPxPUqMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Done\")"
      ],
      "metadata": {
        "id": "FzWTfLDqjukh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot model performance\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs_range = range(1, len(history.epoch) + 1)\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "plt.plot(epochs_range, loss, label='Train Set')\n",
        "plt.plot(epochs_range, val_loss, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Model Loss')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4mdgV9GdyKfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logs"
      ],
      "metadata": {
        "id": "a1wsabkv1kPW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### - 30 Epochs Seems to be enough\n",
        "###### - 32 batch is the maximum number\n",
        "###### - Validation Loss is not a Number !?!?!"
      ],
      "metadata": {
        "id": "fb5ahLBZ1Gfv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FPDwmiVK1K3k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}